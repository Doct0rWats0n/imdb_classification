{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d21d58b-f1f3-4d5c-a006-3565d550d648",
   "metadata": {},
   "source": [
    "# Torch Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025e818-accf-4325-97a4-8e3b5f3c4fd9",
   "metadata": {},
   "source": [
    "torch.autograd - движок автоматического дифференцирования PyTorch, который обеспечивает обучение нейронных сетей.\n",
    "\n",
    "В этом разделе находится концептуальное представление о том, как autograd помогает нейронной сети обучаться."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b5f5c-859b-4bb2-9e7e-b28a75966fe7",
   "metadata": {},
   "source": [
    "## Справка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f3f8a-82f0-42c0-8c64-97be11e6fb8a",
   "metadata": {},
   "source": [
    "Нейронные сети (НС) представляют собой набор вложенных функций, которые выполняются на некоторых входных данных. Эти функции определяются параметрами (состоящими из весов и смещений), которые в PyTorch хранятся в тензорах.\n",
    "\n",
    "Обучение NN происходит в два этапа:\n",
    "\n",
    "Прямое распространение (Forward Propagation): При прямом распространении NN делает свое лучшее предположение о правильном выходе. Для этого он прогоняет входные данные через каждую из своих функций.\n",
    "\n",
    "Обратное распространение: При обратном распространении NN корректирует свои параметры пропорционально ошибке в своем предположении. Для этого он проходит назад от выхода, собирает производные ошибки по параметрам функций (градиенты) и оптимизирует параметры с помощью градиентного спуска. Для более подробного описания backprop посмотрите [это видео от 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d7ef1-c402-4638-b5cf-58c52e6d4d81",
   "metadata": {},
   "source": [
    "## Использование в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddab32-ca5b-4781-a6b8-017bd8c47a9f",
   "metadata": {},
   "source": [
    "Давайте рассмотрим один шаг обучения. Для этого примера мы загрузим предварительно обученную модель resnet18 из torchvision. Мы создаем тензор случайных данных для представления одного изображения с 3 каналами, высотой и шириной 64, и соответствующую метку, инициализированную некоторыми случайными значениями. Метка в предварительно обученных моделях имеет форму (1,1000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdc2a2-3f91-4c29-ad65-d9cf01bd4dfd",
   "metadata": {},
   "source": [
    "Примечание: это руководство работает только на CPU и не сможет сработать на GPU (даже если тензоры переместить на GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702528ae-5953-4df4-a2e3-5e63728ab4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e9ccf-1a81-46cf-810b-297d12538cc9",
   "metadata": {},
   "source": [
    "Далее мы прогоняем входные данные через модель по всем ее слоям, чтобы сделать предсказание. Это прямой проход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d91d1ff0-5285-46d6-b5a1-7b8efc6855dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data)  # forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a05721-0b7d-42c5-b1a8-dcef0a1729e6",
   "metadata": {},
   "source": [
    "Мы используем предсказание модели и соответствующую метку для вычисления ошибки (loss). Следующим шагом будет обратное распространение этой ошибки по сети. Обратное распространение запускается, когда мы вызываем .backward() на тензоре ошибок. Затем Autograd вычисляет и сохраняет градиенты для каждого параметра модели в атрибуте .grad параметра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee173474-ccb7-4084-8179-07b7a4eb9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()  # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e2725-50b3-40e2-97af-a6a645c66756",
   "metadata": {},
   "source": [
    "Далее мы загружаем оптимизатор, в данном случае SGD со скоростью обучения 0.01 и импульсом 0.9. Мы регистрируем все параметры модели в оптимизаторе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29393c41-7dfb-4dba-b4c7-a9b8dfb3d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579b85c-3d43-4c0e-b7a4-2f73d102bbf1",
   "metadata": {},
   "source": [
    "Наконец, мы вызываем .step(), чтобы запустить градиентный спуск. Оптимизатор корректирует каждый параметр по его градиенту, хранящемуся в .grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b396e738-d27a-4cbd-ada2-1f27b00e726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()  # gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d6d672-2427-4487-93a1-86b4a017cb10",
   "metadata": {},
   "source": [
    "На данный момент у нас есть все необходимое для обучения нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe1f11-e7d0-4efa-91ee-7b86a7645816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
